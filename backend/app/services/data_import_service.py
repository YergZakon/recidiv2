"""
–°–µ—Ä–≤–∏—Å –∏–º–ø–æ—Ä—Ç–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel
–ö–†–ò–¢–ò–ß–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –±–µ–∑ –ø–æ—Ç–µ—Ä—å
–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞–º –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
"""
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from sqlalchemy.orm import Session
from sqlalchemy import func
from app.models.real_data import (
    PersonReal, ViolationReal, CrimeTransition, 
    CrimeTimeWindow, RiskAssessmentHistory,
    CRITICAL_TIME_WINDOWS
)
import logging
import json

logger = logging.getLogger(__name__)

class DataImportService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç–∞–Ω—Ç"""
    
    def __init__(self, db: Session):
        self.db = db
        self.data_dir = Path("data")
        self.import_stats = {
            'total_processed': 0,
            'successfully_imported': 0,
            'updated': 0,
            'errors': [],
            'warnings': [],
            'critical_checks': {}
        }
        
    def import_risk_analysis_results(self, filepath: Path = None) -> Dict:
        """
        –ò–º–ø–æ—Ä—Ç RISK_ANALYSIS_RESULTS.xlsx
        –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ 146,570 –∑–∞–ø–∏—Å–µ–π
        """
        
        if not filepath:
            filepath = self.data_dir / "RISK_ANALYSIS_RESULTS.xlsx"
        
        if not filepath.exists():
            raise FileNotFoundError(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–º–ø–æ—Ä—Ç –∏–∑ {filepath}")
        
        try:
            # –ß–∏—Ç–∞–µ–º Excel —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
            df = pd.read_excel(filepath, engine='openpyxl')
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–æ–Ω–∫–∞—Ö
            logger.info(f"–ö–æ–ª–æ–Ω–∫–∏ –≤ —Ñ–∞–π–ª–µ: {list(df.columns)}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            batch_size = 1000
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size]
                self._process_person_batch(batch, filepath.name)
                
                # –ö–æ–º–º–∏—Ç–∏–º –∫–∞–∂–¥—ã–π batch
                self.db.commit()
                logger.info(f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {min(i+batch_size, len(df))}/{len(df)}")
            
            self.import_stats['total_processed'] = len(df)
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞–º –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            self._verify_critical_constants()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
            self.db.rollback()
            raise
        
        return self.import_stats
    
    def _process_person_batch(self, batch: pd.DataFrame, source_file: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –∑–∞–ø–∏—Å–µ–π –æ –ª—é–¥—è—Ö"""
        
        for _, row in batch.iterrows():
            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ, –∞–¥–∞–ø—Ç–∏—Ä—É—è –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∫–æ–ª–æ–Ω–æ–∫
                person_data = self._prepare_person_data(row, source_file)
                
                if not person_data.get('iin'):
                    self.import_stats['errors'].append({
                        'row': _,
                        'error': '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ò–ò–ù'
                    })
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                existing = self.db.query(PersonReal).filter_by(
                    iin=person_data['iin']
                ).first()
                
                if existing:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                    for key, value in person_data.items():
                        setattr(existing, key, value)
                    self.import_stats['updated'] += 1
                else:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                    person = PersonReal(**person_data)
                    self.db.add(person)
                    self.import_stats['successfully_imported'] += 1
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏: {e}")
                self.import_stats['errors'].append({
                    'row': _,
                    'error': str(e),
                    'iin': row.get('–ò–ò–ù', 'Unknown')
                })
    
    def _prepare_person_data(self, row: pd.Series, source_file: str) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ë–î —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º pandas –æ–±—ä–µ–∫—Ç—ã –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ —Ç–∏–ø—ã
        raw_dict = row.to_dict()
        serializable_dict = {}
        for key, value in raw_dict.items():
            if pd.isna(value):
                serializable_dict[key] = None
            elif hasattr(value, 'isoformat'):  # datetime/Timestamp
                serializable_dict[key] = value.isoformat()
            else:
                serializable_dict[key] = value
        
        person_data = {
            'source_file': source_file,
            'import_date': datetime.utcnow(),
            'raw_data': serializable_dict
        }
        
        # –ú–∞–ø–ø–∏–Ω–≥ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
        column_mappings = {
            'iin': ['–ò–ò–ù', 'IIN', 'iin', 'person_iin'],
            'last_name': ['–§–∞–º–∏–ª–∏—è', 'last_name', 'surname'],
            'first_name': ['–ò–º—è', 'first_name', 'name'],
            'middle_name': ['–û—Ç—á–µ—Å—Ç–≤–æ', 'middle_name', 'patronymic'],
            'current_age': ['current_age', 'age', '–í–æ–∑—Ä–∞—Å—Ç'],
            'gender': ['gender', '–ü–æ–ª', 'sex'],
            'region': ['region', '–†–µ–≥–∏–æ–Ω', 'Oblast'],
            'pattern_type': ['pattern_type', 'behavior_pattern', 'Pattern'],
            'total_cases': ['total_cases', 'total_violations', '–í—Å–µ–≥–æ –¥–µ–ª'],
            'criminal_count': ['criminal_count', 'criminal_cases', '–£–≥–æ–ª–æ–≤–Ω—ã–µ'],
            'admin_count': ['admin_count', 'admin_cases', '–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ'],
            'risk_total_risk_score': ['risk_total_risk_score', 'risk_score', 'total_risk_score', 'Risk Score'],
            'days_since_last': ['days_since_last', 'days_since_last_violation'],
            'has_property': ['has_property', 'property_owner'],
            'has_job': ['has_job', 'employed'],
            'has_family': ['has_family', 'married'],
        }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
        for db_field, excel_columns in column_mappings.items():
            for excel_col in excel_columns:
                if excel_col in row.index and pd.notna(row[excel_col]):
                    value = row[excel_col]
                    
                    # –û—á–∏—Å—Ç–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
                    if db_field == 'iin':
                        # –û—á–∏—â–∞–µ–º –ò–ò–ù –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                        value = str(value).replace('-', '').replace(' ', '').strip()
                    elif db_field in ['current_age', 'total_cases', 'criminal_count', 'admin_count', 'days_since_last']:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞
                        try:
                            value = int(float(value))
                        except:
                            value = 0
                    elif db_field == 'risk_total_risk_score':
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float
                        try:
                            value = float(value)
                        except:
                            value = 0.0
                    elif db_field in ['has_property', 'has_job', 'has_family']:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 0/1
                        value = 1 if value in [1, '1', 'Yes', '–î–∞', True] else 0
                    
                    person_data[db_field] = value
                    break
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω–æ–µ –§–ò–û
        if all(k in person_data for k in ['last_name', 'first_name']):
            person_data['full_name'] = f"{person_data.get('last_name', '')} {person_data.get('first_name', '')} {person_data.get('middle_name', '')}".strip()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä–∏—Å–∫–∞
        risk_score = person_data.get('risk_total_risk_score', 0)
        if risk_score >= 7:
            person_data['risk_category'] = 'critical'
        elif risk_score >= 5:
            person_data['risk_category'] = 'high'
        elif risk_score >= 3:
            person_data['risk_category'] = 'medium'
        else:
            person_data['risk_category'] = 'low'
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        required_fields = ['iin', 'last_name', 'first_name', 'current_age', 'gender', 
                          'total_cases', 'risk_total_risk_score']
        filled_fields = sum(1 for f in required_fields if person_data.get(f))
        person_data['data_quality_score'] = filled_fields / len(required_fields)
        
        return person_data
    
    def _verify_critical_constants(self):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç–∞–Ω—Ç –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        –ö–†–ò–¢–ò–ß–ù–û: –î–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º!
        """
        
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç–∞–Ω—Ç...")
        
        # 1. –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~146,570)
        total_count = self.db.query(PersonReal).count()
        expected_total = 146570
        diff_percent = abs(total_count - expected_total) / expected_total * 100
        
        if diff_percent < 1:  # –î–æ–ø—É—Å–∫ 1%
            logger.info(f"‚úÖ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count:,} (–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_total:,})")
            self.import_stats['critical_checks']['total_count'] = 'PASS'
        else:
            logger.warning(f"‚ö†Ô∏è –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count:,} (–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_total:,}, —Ä–∞–∑–Ω–∏—Ü–∞ {diff_percent:.1f}%)")
            self.import_stats['warnings'].append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∞ {diff_percent:.1f}%")
            self.import_stats['critical_checks']['total_count'] = 'WARNING'
        
        # 2. –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å ~72.7%)
        unstable_count = self.db.query(PersonReal).filter(
            PersonReal.pattern_type == 'mixed_unstable'
        ).count()
        
        if total_count > 0:
            unstable_percent = (unstable_count / total_count) * 100
            expected_percent = 72.7
            
            if abs(unstable_percent - expected_percent) < 5:  # –î–æ–ø—É—Å–∫ 5%
                logger.info(f"‚úÖ –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {unstable_percent:.1f}% (–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_percent}%)")
                self.import_stats['critical_checks']['unstable_pattern'] = 'PASS'
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {unstable_percent:.1f}% (–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_percent}%)")
                self.import_stats['warnings'].append(f"–ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {unstable_percent:.1f}%")
                self.import_stats['critical_checks']['unstable_pattern'] = 'WARNING'
        
        # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—Ü–∏–¥–∏–≤–∏—Å—Ç–æ–≤ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~12,333)
        recidivists = self.db.query(PersonReal).filter(
            PersonReal.total_cases > 1
        ).count()
        expected_recidivists = 12333
        
        if abs(recidivists - expected_recidivists) < 500:  # –î–æ–ø—É—Å–∫ 500
            logger.info(f"‚úÖ –†–µ—Ü–∏–¥–∏–≤–∏—Å—Ç–æ–≤: {recidivists:,} (–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_recidivists:,})")
            self.import_stats['critical_checks']['recidivists'] = 'PASS'
        else:
            logger.warning(f"‚ö†Ô∏è –†–µ—Ü–∏–¥–∏–≤–∏—Å—Ç–æ–≤: {recidivists:,} (–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_recidivists:,})")
            self.import_stats['warnings'].append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—Ü–∏–¥–∏–≤–∏—Å—Ç–æ–≤: {recidivists:,}")
            self.import_stats['critical_checks']['recidivists'] = 'WARNING'
        
        # 4. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ä–∏—Å–∫–∞
        risk_distribution = self.db.query(
            PersonReal.risk_category,
            func.count(PersonReal.id)
        ).group_by(PersonReal.risk_category).all()
        
        logger.info("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ä–∏—Å–∫–∞:")
        for category, count in risk_distribution:
            percent = (count / total_count * 100) if total_count > 0 else 0
            logger.info(f"  - {category}: {count:,} ({percent:.1f}%)")
        
        # 5. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–∏—Å–∫ (7+)
        critical_risk = self.db.query(PersonReal).filter(
            PersonReal.risk_total_risk_score >= 7
        ).count()
        critical_percent = (critical_risk / total_count * 100) if total_count > 0 else 0
        logger.info(f"üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–∏—Å–∫ (7+): {critical_risk:,} ({critical_percent:.1f}%)")
    
    def import_crime_transitions(self, filepath: Path = None) -> Dict:
        """
        –ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –æ –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö –∞–¥–º–∏–Ω->—É–≥–æ–ª–æ–≤–∫–∞
        –ö–†–ò–¢–ò–ß–ù–û: 6,465 –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∞–¥–º–∏–Ω->–∫—Ä–∞–∂–∞
        """
        
        if not filepath:
            filepath = self.data_dir / "crime_analysis_results.xlsx"
        
        if not filepath.exists():
            logger.warning(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return {'status': 'file_not_found'}
        
        logger.info(f"üìä –ò–º–ø–æ—Ä—Ç –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∏–∑ {filepath}")
        
        try:
            # –ß–∏—Ç–∞–µ–º –ª–∏—Å—Ç —Å —ç—Å–∫–∞–ª–∞—Ü–∏–µ–π
            df = pd.read_excel(filepath, sheet_name="–≠—Å–∫–∞–ª–∞—Ü–∏—è")
            
            for _, row in df.iterrows():
                transition_data = {
                    'admin_violation': row.get('–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ', row.get('admin_type')),
                    'criminal_offense': row.get('–£–≥–æ–ª–æ–≤–Ω–æ–µ', row.get('criminal_type')),
                    'transition_count': int(row.get('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤', row.get('count', 0))),
                    'avg_days': float(row.get('–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è', row.get('avg_days', 0))),
                    'min_days': int(row.get('–ú–∏–Ω –≤—Ä–µ–º—è', row.get('min_days', 0))),
                    'max_days': int(row.get('–ú–∞–∫—Å –≤—Ä–µ–º—è', row.get('max_days', 0))),
                    'preventable_percent': float(row.get('–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏–º–æ—Å—Ç—å', row.get('preventable', 0))),
                    'source_file': filepath.name,
                    'import_date': datetime.utcnow()
                }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                existing = self.db.query(CrimeTransition).filter_by(
                    admin_violation=transition_data['admin_violation'],
                    criminal_offense=transition_data['criminal_offense']
                ).first()
                
                if existing:
                    for key, value in transition_data.items():
                        setattr(existing, key, value)
                else:
                    transition = CrimeTransition(**transition_data)
                    self.db.add(transition)
            
            self.db.commit()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É
            admin_to_theft = self.db.query(CrimeTransition).filter(
                CrimeTransition.criminal_offense.like('%–∫—Ä–∞–∂–∞%')
            ).all()
            
            total_transitions = sum(t.transition_count for t in admin_to_theft)
            logger.info(f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∞–¥–º–∏–Ω->–∫—Ä–∞–∂–∞: {total_transitions:,} (–æ–∂–∏–¥–∞–ª–æ—Å—å 6,465)")
            
            if abs(total_transitions - 6465) > 100:
                logger.warning(f"‚ö†Ô∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∞–¥–º–∏–Ω->–∫—Ä–∞–∂–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ")
            
            return {'status': 'success', 'imported': len(df)}
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤: {e}")
            self.db.rollback()
            return {'status': 'error', 'error': str(e)}
    
    def import_time_windows(self):
        """–ò–º–ø–æ—Ä—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        
        logger.info("‚è∞ –ò–º–ø–æ—Ä—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω...")
        
        for crime_type, days in CRITICAL_TIME_WINDOWS.items():
            existing = self.db.query(CrimeTimeWindow).filter_by(
                crime_type=crime_type
            ).first()
            
            if not existing:
                window = CrimeTimeWindow(
                    crime_type=crime_type,
                    window_days=days,
                    median_days=days,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –º–µ–¥–∏–∞–Ω—É
                    preventability_score=97.0 if crime_type != "–£–±–∏–π—Å—Ç–≤–æ" else 82.3,  # –ò–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                    source="research_2024"
                )
                self.db.add(window)
        
        self.db.commit()
        logger.info(f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(CRITICAL_TIME_WINDOWS)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω")
    
    def get_import_summary(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
        
        summary = {
            'persons': {
                'total': self.db.query(PersonReal).count(),
                'with_high_risk': self.db.query(PersonReal).filter(
                    PersonReal.risk_total_risk_score >= 7
                ).count(),
                'recidivists': self.db.query(PersonReal).filter(
                    PersonReal.total_cases > 1
                ).count()
            },
            'transitions': {
                'total': self.db.query(CrimeTransition).count(),
                'admin_to_theft': self.db.query(CrimeTransition).filter(
                    CrimeTransition.criminal_offense.like('%–∫—Ä–∞–∂–∞%')
                ).count()
            },
            'time_windows': self.db.query(CrimeTimeWindow).count(),
            'import_stats': self.import_stats
        }
        
        return summary